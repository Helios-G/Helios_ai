import os
# Mac ì¤‘ë³µ ë¡œë“œ ì—ëŸ¬ ë°©ì§€
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
from torchvision import models
import onnx
from onnx_tf.backend import prepare

# 1. ëª¨ë¸ êµ¬ì¡° ì •ì˜ (DenseNet121)
class CheXpertModel(nn.Module):
    def __init__(self, num_classes=14):
        super(CheXpertModel, self).__init__()
        # pretrained=Falseë¡œ ì„¤ì • (êµ¬ì¡°ë§Œ ê°€ì ¸ì˜´)
        self.model = models.densenet121(pretrained=False)
        num_ftrs = self.model.classifier.in_features
        self.model.classifier = nn.Linear(num_ftrs, num_classes)

    def forward(self, x):
        return self.model(x)

# ==========================================
# 2. ëª¨ë¸ ì´ˆê¸°í™” (ì´ ë¶€ë¶„ì´ ë¹ ì ¸ì„œ ì—ëŸ¬ê°€ ë‚¬ë˜ ê²ƒì…ë‹ˆë‹¤!)
# ==========================================
model = CheXpertModel(num_classes=14)

# 3. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
# ê²½ë¡œê°€ ë§ëŠ”ì§€ ê¼­ í™•ì¸í•˜ì„¸ìš”!
checkpoint_path = '/Users/kimseonmin/HELIOS/federated/Chexpert/config/pre_train.pth' 
device = torch.device("cpu")

print(f"ğŸ“‚ ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì¤‘: {checkpoint_path}")

if not os.path.exists(checkpoint_path):
    print(f"âŒ ì—ëŸ¬: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ -> {checkpoint_path}")
    exit()

checkpoint = torch.load(checkpoint_path, map_location=device)

# 4. íŒŒì¼ êµ¬ì¡°ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
    print("âœ… êµ¬ì¡° í™•ì¸: Type A ('state_dict' í‚¤ í¬í•¨)")
    state_dict = checkpoint['state_dict']
elif isinstance(checkpoint, dict) and 'model' in checkpoint:
    print("âœ… êµ¬ì¡° í™•ì¸: Type B ('model' í‚¤ í¬í•¨)")
    state_dict = checkpoint['model']
else:
    print("âœ… êµ¬ì¡° í™•ì¸: Type C (ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ ì§ì ‘ ë¡œë“œ)")
    state_dict = checkpoint

# í‚¤ ì´ë¦„ ë³€ê²½ ('module.' ì ‘ë‘ì‚¬ ì œê±°)
new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}

# 5. ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
try:
    model.load_state_dict(new_state_dict)
    print("ğŸ‰ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ!")
except Exception as e:
    print(f"âŒ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨ (1ì°¨ ì‹œë„): {e}")
    print("âš ï¸ strict=Falseë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
    try:
        model.load_state_dict(new_state_dict, strict=False)
        print("ğŸ‰ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ (strict=False)!")
    except Exception as e2:
        print(f"âŒ ìµœì¢… ì‹¤íŒ¨: {e2}")
        exit()

model.eval()

# 6. ONNXë¡œ ë³€í™˜
print("ğŸ”„ ONNX ë³€í™˜ ì‹œì‘...")
dummy_input = torch.randn(1, 3, 320, 320) # CheXpert ì…ë ¥ í¬ê¸°
onnx_path = "chexpert.onnx"
torch.onnx.export(model, dummy_input, onnx_path, verbose=False, input_names=['input'], output_names=['output'], opset_version=11)
print("âœ… ONNX íŒŒì¼ ìƒì„± ì™„ë£Œ")

# 7. ONNX -> TensorFlow SavedModel
print("ğŸ”„ TensorFlow SavedModel ë³€í™˜ ì‹œì‘...")
onnx_model = onnx.load(onnx_path)
tf_rep = prepare(onnx_model)
tf_rep.export_graph("chexpert_saved_model")
print("âœ… TensorFlow SavedModel ë³€í™˜ ì™„ë£Œ!")

# 8. ì•ˆë‚´ ë©”ì‹œì§€
print("\n" + "="*50)
print("ğŸš€ ë³€í™˜ì´ ê±°ì˜ ëë‚¬ìŠµë‹ˆë‹¤! ì•„ë˜ ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì— ì…ë ¥í•˜ì„¸ìš”:")
print("="*50)
print("tensorflowjs_converter --input_format=tf_saved_model --output_node_names='output' --saved_model_tags=serve ./chexpert_saved_model ./chexpert_tfjs")
print("="*50)